{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learned:\n",
    "- Broadcasting: essentially, by adding dimension to dataset, np infers calculation and fills in gaps, add dimension where you want the iteration, so for X - u, want delta between each feature vector and means of all the classes, so add dimension to rows, as now each row is 2D matrix of differences\n",
    "- np has some super useful interactions, usually way to get around iterating\n",
    "- NB good intro/first stab at problem\n",
    "- Without broadcasting, easier to understand, but computationally less efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNB:\n",
    "    def __init__(self, var_smoothing = 1e-9):\n",
    "        #set var smoothing\n",
    "        self.var_smoothing = var_smoothing\n",
    "\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        #convert X, Y df's to numpy arrays\n",
    "        X_train = X.to_numpy()\n",
    "        Y_train = Y.to_numpy()\n",
    "\n",
    "        #identify classes and #\n",
    "        self.classes = np.unique(Y_train)\n",
    "        self.num_classes = len(np.unique(Y_train))\n",
    "\n",
    "        #initialize means, vars, and priors into matrix\n",
    "        self.means = np.zeros((self.num_classes,X.shape[1]))\n",
    "        self.vars = np.zeros((self.num_classes,X.shape[1]))\n",
    "        self.class_priors = np.zeros(self.num_classes)\n",
    "\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            #for each class, take input data\n",
    "            X_cls = X[Y == cls]\n",
    "\n",
    "            #for corresponding index to class, take the mean of each col\n",
    "            self.means[i, :] = np.mean(X_cls, axis=0)\n",
    "            #take variance and add smoohting to ensure numerical stability\n",
    "            self.vars[i, :] = np.var(X_cls, axis=0)\n",
    "            #proportion of data in class / total data\n",
    "            self.class_priors[i] = len(X_cls) / len(X)\n",
    "            \n",
    "        self.vars += self.var_smoothing\n",
    "        \n",
    "    def predict(self, X):\n",
    "        #convert to numpy array\n",
    "        X_pred = X.to_numpy()\n",
    "\n",
    "        #calculate log probabilities for numerical stability and computational efficiency\n",
    "        #P(Y)\n",
    "        log_class_priors = np.log(self.class_priors)\n",
    "        #use broadcasting and matrix/vector operations to avoid hassle\n",
    "        log_likelihoods = -0.5 * (np.log(2 * np.pi * self.vars) + ((X_pred[:, np.newaxis, :] - self.means) ** 2 / self.vars))\n",
    "\n",
    "        #sum log likelihoods across features, so 2D matrix, each row is datum, each is likelihood for the class\n",
    "        #P(X|Y)\n",
    "        log_likelihoods = np.sum(log_likelihoods, axis=2)\n",
    "        \n",
    "        #calculate posterior probabilities\n",
    "        log_posteriors = log_class_priors + log_likelihoods\n",
    "\n",
    "        #take arg max across rows, so find column index that yields max value, then find class\n",
    "        return self.classes[np.argmax(log_posteriors, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDA:\n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        # set var smoothing\n",
    "        self.var_smoothing = var_smoothing\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        X_train = X.to_numpy()\n",
    "        Y_train = Y.to_numpy()\n",
    "\n",
    "        self.classes = np.unique(Y_train)\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.num_features = X_train.shape[1]\n",
    "\n",
    "        #set parameters, include var smoothing\n",
    "        self.means_matrix = np.zeros((self.num_classes, self.num_features))\n",
    "        self.cov_matrix = np.zeros((self.num_features, self.num_features))\n",
    "        self.class_priors = np.zeros(self.num_classes)\n",
    "\n",
    "        #calculate means, priors, and variance across classes\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            X_cls = X_train[Y_train == cls]\n",
    "            class_mean = np.mean(X_cls, axis=0)\n",
    "            self.means_matrix[i, :] = class_mean\n",
    "            self.class_priors[i] = X_cls.shape[0] / X_train.shape[0]\n",
    "\n",
    "            self.cov_matrix += ((X_cls - class_mean).T @ (X_cls - class_mean))\n",
    "\n",
    "        #normalize cov matrix, add smoothing\n",
    "        self.cov_matrix /= X_train.shape[0]\n",
    "        self.cov_matrix += self.var_smoothing * np.eye(self.num_features)\n",
    "\n",
    "    #calculates P(X|Y) based on multidimensional gaussian, then returns class with highest prob\n",
    "    #does it class by class, no broadcasting\n",
    "    def predict(self, X):\n",
    "        X = X.to_numpy()  # Convert to numpy array\n",
    "        log_likelihoods = np.zeros((X.shape[0], self.num_classes))\n",
    "\n",
    "        # Precompute determinant and inverse of the shared covariance matrix\n",
    "        det_cov_matrix = np.linalg.det(self.cov_matrix)\n",
    "        inv_cov_matrix = np.linalg.inv(self.cov_matrix)\n",
    "\n",
    "        # Iterate over each class\n",
    "        for i in range(self.num_classes):\n",
    "            mu_cls = self.means_matrix[i]  # Mean for class i\n",
    "            prior_cls = self.class_priors[i]  # Prior for class i\n",
    "\n",
    "            # Compute the difference between X and the class mean\n",
    "            diff = X - mu_cls\n",
    "\n",
    "            # Compute the quadratic form: (X - mu_cls) @ inv_cov_matrix @ (X - mu_cls)^T\n",
    "            quadratic_form = np.einsum('ij,jk,ik->i', diff, inv_cov_matrix, diff)\n",
    "\n",
    "            # Compute log-likelihood for class i\n",
    "            log_likelihood = -0.5 * quadratic_form + np.log(prior_cls)\n",
    "            log_likelihoods[:, i] = log_likelihood\n",
    "\n",
    "        # Return the class with the highest log-likelihood\n",
    "        predicted_class = np.argmax(log_likelihoods, axis=1)\n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB 0.639344262295082\n",
      "GDA 0.7049180327868853\n"
     ]
    }
   ],
   "source": [
    "my_GNB = GNB()\n",
    "\n",
    "df = pd.read_csv(\"heart.csv\")\n",
    "X = df[['trtbps','chol']]\n",
    "Y = df[\"output\"]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "my_GNB.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = my_GNB.predict(X_test)\n",
    "accuracy = accuracy_score(Y_pred, Y_test)\n",
    "print(\"GNB\",accuracy)\n",
    "\n",
    "my_GDA = GDA()\n",
    "\n",
    "df = pd.read_csv(\"heart.csv\")\n",
    "X = df[['trtbps','chol']]\n",
    "Y = df[\"output\"]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "my_GDA.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = my_GDA.predict(X_test)\n",
    "accuracy = accuracy_score(Y_pred, Y_test)\n",
    "print(\"GDA\",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
