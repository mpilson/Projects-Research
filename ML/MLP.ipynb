{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- computational graph?\n",
    "- dimensions\n",
    "\n",
    "Learned:\n",
    "- numpy arrays passed into ML models, not df's\n",
    "- standard dimensions are W (n_out, n_in) or (neurons,features), so transpose X, typical\n",
    "- Use dictionaries to cache values and access activation/cost functions their and grads\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred,Y):\n",
    "    return ((y_pred - Y)**2)/Y.shape[0]\n",
    "\n",
    "def MSE_grad(y_pred,Y):\n",
    "    return y_pred - Y\n",
    "\n",
    "def BCE(y_pred, Y):\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return - (1/Y.shape[0]) * np.sum(Y * np.log(y_pred) + (1 - Y) * np.log(1 - y_pred))\n",
    "\n",
    "def BCE_grad(y_pred, Y):\n",
    "    return y_pred - Y\n",
    "\n",
    "def CCE(y_pred,Y):\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    num_classes = y_pred.shape[1]\n",
    "    Y_labels = np.eye(num_classes)[Y]\n",
    "    return -(1 / Y.shape[0]) * np.sum(Y_labels * np.log(y_pred))\n",
    "\n",
    "def CCE_grad(y_pred,Y):\n",
    "    num_classes = y_pred.shape[1]\n",
    "    Y_labels = np.eye(num_classes)[Y]\n",
    "    return y_pred - Y_labels\n",
    "\n",
    "cost_grad = {MSE: MSE_grad, BCE: BCE_grad, CCE: CCE_grad}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation functions and grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def ReLU_grad(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_grad(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def Tanh(z):\n",
    "    return (2 / (1 + np.exp(-2*z))) - 1\n",
    "\n",
    "def Tanh_grad(z):\n",
    "    return 1-(Tanh(z))**2\n",
    "\n",
    "activation_grad = {ReLU: ReLU_grad, sigmoid: sigmoid_grad, Tanh: Tanh_grad}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xavier initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_unif(n_out, n_in):\n",
    "    limit = np.sqrt(6 / (n_out + n_in))\n",
    "    weights = np.random.uniform(-limit, limit, size=(n_out, n_in))\n",
    "    return weights\n",
    "\n",
    "def xavier_norm(n_out, n_in):\n",
    "    stddev = np.sqrt(2 / (n_out + n_in))\n",
    "    weights = np.random.normal(0, stddev, size=(n_out, n_in))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HE initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HE_unif(n_out, n_in):\n",
    "    limit = np.sqrt(6 / (n_in))\n",
    "    weights = np.random.uniform(-limit, limit, size=(n_out, n_in))\n",
    "    return weights\n",
    "\n",
    "def HE_norm(n_out, n_in):\n",
    "    stddev = 2/n_in\n",
    "    weights = np.random.normal(0, stddev, size=(n_out,n_in))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self,learning_rate = .0001, max_epochs = 500, batch_size = 64, convergence_level = 1e-5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.convergence_level = convergence_level\n",
    "        self.cache = None\n",
    "        self.cost_storage = []\n",
    "        self.layers = []\n",
    "    \n",
    "    def fit(self, X_train, Y_train, layer_sizes, activation = ReLU, init_method = HE_norm, cost_function = CCE):\n",
    "        self.cache = {}\n",
    "        self.activation = activation\n",
    "        self.init = init_method\n",
    "        self.cost_function = cost_function\n",
    "        \n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            size_in = layer_sizes[i]\n",
    "            size_out = layer_sizes[i+1]\n",
    "            new_layer = self.layer(size_in,size_out)\n",
    "            new_layer.init_W(self.init)\n",
    "            new_layer.init_b()\n",
    "            self.layers.append(new_layer)\n",
    "        for epoch in range(self.max_epochs):\n",
    "            print(\"epoch\",epoch)\n",
    "            shuffled_indices = np.random.permutation(len(X_train))\n",
    "            X_shuffled = X_train[shuffled_indices]\n",
    "            Y_shuffled = Y_train[shuffled_indices]\n",
    "            \n",
    "            epoch_cost = 0\n",
    "            for i in range(0, X_train.shape[0], self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size,:]\n",
    "                Y_batch = Y_shuffled[i:i+self.batch_size]\n",
    "                y_pred_batch = self.forward_pass(X_batch)\n",
    "                \n",
    "                batch_cost = np.sum(self.cost_function(y_pred_batch,Y_batch), axis = 0)\n",
    "                epoch_cost += batch_cost\n",
    "                \n",
    "                self.back_prop(y_pred_batch,Y_batch)\n",
    "            \n",
    "            epoch_cost /= X_train.shape[0]\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(float(epoch_cost))\n",
    "                self.cost_storage.append(float(epoch_cost))\n",
    "            \n",
    "            if len(self.cost_storage) >= 2:\n",
    "                cost_prev = self.cost_storage[-2]\n",
    "                cost_curr = self.cost_storage[-1]\n",
    "                if abs(cost_prev - cost_curr) < self.convergence_level:\n",
    "                    return\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_pred = self.forward_pass(X_test)\n",
    "        return np.argmax(y_pred, axis = 1)\n",
    "            \n",
    "    def forward_pass(self,X_batch):\n",
    "        X_batch = X_batch.T\n",
    "        A_prev = X_batch\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            W = layer.W\n",
    "            b = layer.b\n",
    "            Z = (W @ A_prev) + b\n",
    "            A = Z\n",
    "            if i != len(self.layers) - 1:\n",
    "                A = self.activation(Z)\n",
    "            self.cache[f\"layer_{i+1}\"] = {\"A\": A, \"Z\": Z, \"A_prev\": A_prev}\n",
    "            A_prev = A\n",
    "        y_pred = A.T\n",
    "        return y_pred\n",
    "\n",
    "    def back_prop(self, y_pred, Y):\n",
    "        batch_size = Y.shape[0]\n",
    "        \n",
    "        l_grad = cost_grad[self.cost_function]\n",
    "        a_grad = activation_grad[self.activation]\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            index = len(self.layers)- i\n",
    "            Z = self.cache[f\"layer_{index}\"]['Z']\n",
    "            A_prev = self.cache[f\"layer_{index}\"][\"A_prev\"]\n",
    "            layer = self.layers[index-1]\n",
    "            \n",
    "            if i == 0:\n",
    "                loss = l_grad(y_pred, Y).T\n",
    "                dZ = (loss) * a_grad(Z)\n",
    "            else:\n",
    "                dZ = (W_prev.T @ dZ) * a_grad(Z)\n",
    "                \n",
    "            W, b = layer.W, layer.b\n",
    "            W_prev = W\n",
    "            \n",
    "            dW = (dZ @ A_prev.T) / batch_size\n",
    "            db = np.sum(dZ, axis=1) / batch_size\n",
    "            db = db.reshape(db.shape[0],1)      \n",
    "            \n",
    "            W -= self.learning_rate * dW\n",
    "            b -= self.learning_rate * db\n",
    "            layer.W, layer.b = W, b\n",
    "            \n",
    "    def plot_cost(self):\n",
    "        plt.plot(self.cost_storage)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Cost over Epochs')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "            \n",
    "    class layer:\n",
    "        def __init__(self,n_in,n_out):\n",
    "            self.W = None\n",
    "            self.b = None\n",
    "            self.n_in = n_in\n",
    "            self.n_out = n_out\n",
    "        \n",
    "        def init_W(self, init_method):\n",
    "            self.W = init_method(self.n_out,self.n_in)       \n",
    "            \n",
    "        def init_b(self):\n",
    "            self.b = np.zeros((self.n_out,1))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "epoch 0\n",
      "0.07043457093766678\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "[0.07043457093766678]\n",
      "Accuracy: 0.90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method MLP.plot_cost of <__main__.MLP object at 0x11b58d590>>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"mnist_train.csv\")\n",
    "df_test = pd.read_csv(\"mnist_test.csv\")\n",
    "X_train, Y_train = df_train.drop(columns='label'), df_train['label']\n",
    "X_test, Y_test = df_test.drop(columns='label'), df_test['label']\n",
    "\n",
    "X_train, Y_train = X_train.to_numpy(), Y_train.to_numpy()\n",
    "X_test, Y_test = X_test.to_numpy(), Y_test.to_numpy()\n",
    "\n",
    "\n",
    "my_MLP = MLP(max_epochs = 5, learning_rate = .0001)\n",
    "layers = [784, 512, 256, 10]\n",
    "my_MLP.fit(X_train, Y_train, layers)\n",
    "\n",
    "print(my_MLP.cost_storage)\n",
    "\n",
    "y_pred = my_MLP.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "my_MLP.plot_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 0.6926\n",
      "Epoch [1/5], Step [200/938], Loss: 0.2916\n",
      "Epoch [1/5], Step [300/938], Loss: 0.2346\n",
      "Epoch [1/5], Step [400/938], Loss: 0.2016\n",
      "Epoch [1/5], Step [500/938], Loss: 0.1916\n",
      "Epoch [1/5], Step [600/938], Loss: 0.1443\n",
      "Epoch [1/5], Step [700/938], Loss: 0.1509\n",
      "Epoch [1/5], Step [800/938], Loss: 0.1278\n",
      "Epoch [1/5], Step [900/938], Loss: 0.1237\n",
      "Epoch [2/5], Step [100/938], Loss: 0.0903\n",
      "Epoch [2/5], Step [200/938], Loss: 0.0960\n",
      "Epoch [2/5], Step [300/938], Loss: 0.0891\n",
      "Epoch [2/5], Step [400/938], Loss: 0.0809\n",
      "Epoch [2/5], Step [500/938], Loss: 0.0919\n",
      "Epoch [2/5], Step [600/938], Loss: 0.0767\n",
      "Epoch [2/5], Step [700/938], Loss: 0.0892\n",
      "Epoch [2/5], Step [800/938], Loss: 0.0854\n",
      "Epoch [2/5], Step [900/938], Loss: 0.0828\n",
      "Epoch [3/5], Step [100/938], Loss: 0.0484\n",
      "Epoch [3/5], Step [200/938], Loss: 0.0614\n",
      "Epoch [3/5], Step [300/938], Loss: 0.0655\n",
      "Epoch [3/5], Step [400/938], Loss: 0.0527\n",
      "Epoch [3/5], Step [500/938], Loss: 0.0547\n",
      "Epoch [3/5], Step [600/938], Loss: 0.0548\n",
      "Epoch [3/5], Step [700/938], Loss: 0.0571\n",
      "Epoch [3/5], Step [800/938], Loss: 0.0508\n",
      "Epoch [3/5], Step [900/938], Loss: 0.0637\n",
      "Epoch [4/5], Step [100/938], Loss: 0.0318\n",
      "Epoch [4/5], Step [200/938], Loss: 0.0349\n",
      "Epoch [4/5], Step [300/938], Loss: 0.0447\n",
      "Epoch [4/5], Step [400/938], Loss: 0.0416\n",
      "Epoch [4/5], Step [500/938], Loss: 0.0407\n",
      "Epoch [4/5], Step [600/938], Loss: 0.0538\n",
      "Epoch [4/5], Step [700/938], Loss: 0.0457\n",
      "Epoch [4/5], Step [800/938], Loss: 0.0562\n",
      "Epoch [4/5], Step [900/938], Loss: 0.0390\n",
      "Epoch [5/5], Step [100/938], Loss: 0.0187\n",
      "Epoch [5/5], Step [200/938], Loss: 0.0283\n",
      "Epoch [5/5], Step [300/938], Loss: 0.0320\n",
      "Epoch [5/5], Step [400/938], Loss: 0.0251\n",
      "Epoch [5/5], Step [500/938], Loss: 0.0414\n",
      "Epoch [5/5], Step [600/938], Loss: 0.0277\n",
      "Epoch [5/5], Step [700/938], Loss: 0.0335\n",
      "Epoch [5/5], Step [800/938], Loss: 0.0376\n",
      "Epoch [5/5], Step [900/938], Loss: 0.0343\n",
      "Finished Training\n",
      "Test Accuracy of the model on the 10,000 test images: 97.98%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Reshape and normalize images, convert labels to tensors\n",
    "X_train = torch.tensor(X_train.reshape(-1, 28*28) / 255.0, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test.reshape(-1, 28*28) / 255.0, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28\n",
    "hidden1_size = 512\n",
    "hidden2_size = 256\n",
    "output_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = MLP(input_size, hidden1_size, hidden2_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy of the model on the 10,000 test images: {accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
