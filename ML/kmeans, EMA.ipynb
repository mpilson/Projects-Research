{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learned:\n",
    "- another appearance of broadcasting\n",
    "- more numpy methods\n",
    "- indentation/spacing customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WCSS cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WCSS(X, curr_clusters, centroids):\n",
    "\ttotal_cost = 0\n",
    "\tfor i, centroid in enumerate(centroids):\n",
    "\t\t#slice data set to get data for current cluster\n",
    "\t\tcluster_data = X[curr_clusters == i]\n",
    "\t\t#compute normalized distance between datum and centroid,\n",
    "\t\tdistances = np.linalg.norm(cluster_data - centroid, axis=1)**2\n",
    "\t\t#add to total\n",
    "\t\ttotal_cost += np.sum(distances)\n",
    "\treturn total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, max_epochs=100):\n",
    "        # Set model parameters\n",
    "        self.max_epochs = max_epochs\n",
    "        self.centroids = None\n",
    "        self.cost_storage = []\n",
    "\n",
    "    def fit(self, X_train, k):\n",
    "        # Convert to numpy array, choose k data points to initialize centroids\n",
    "        X_train = X_train.to_numpy()\n",
    "        random_indices = np.random.choice(X_train.shape[0], size=k, replace=False)\n",
    "        self.centroids = X_train[random_indices]\n",
    "\n",
    "        for epoch in range(self.max_epochs):\n",
    "            prev_centroids = self.centroids\n",
    "            # Calculate distances between each point and the centroids\n",
    "            curr_distances = np.linalg.norm(X_train[:, np.newaxis, :] - self.centroids, axis=2)\n",
    "            curr_clusters = np.argmin(curr_distances, axis=1)\n",
    "\n",
    "            # Initialize new centroids array\n",
    "            new_centroids = np.zeros_like(self.centroids)\n",
    "            \n",
    "            # Update centroids using an external for loop\n",
    "            for i in range(k):\n",
    "                points_in_cluster = X_train[curr_clusters == i]\n",
    "                \n",
    "                if len(points_in_cluster) > 0:  # Avoid empty clusters\n",
    "                    new_centroids[i] = np.mean(points_in_cluster, axis=0)\n",
    "                else:\n",
    "                    new_centroids[i] = self.centroids[i]  # Keep previous centroid if cluster is empty\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "            # Compute the current cost (WCSS) and store it\n",
    "            curr_cost = WCSS(X_train, curr_clusters, self.centroids)\n",
    "            self.cost_storage.append(curr_cost)\n",
    "\n",
    "            if np.allclose(self.centroids, prev_centroids, atol=1e-6):\n",
    "                break\n",
    "\n",
    "        # Final cluster assignments\n",
    "        self.clusters = curr_clusters\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_test = X_test.to_numpy()\n",
    "        distances = np.linalg.norm(X_test[:, np.newaxis] - self.centroids, axis=2)\n",
    "        clusters = np.argmin(distances, axis=1)\n",
    "        return clusters\n",
    "    \n",
    "    #return centroids\n",
    "    def get_centroids(self):\n",
    "        return self.centroids\n",
    "    \n",
    "    def get_cov_matrices(self, X_train, k):\n",
    "        covariances = []\n",
    "        for i in range(k):\n",
    "            # Get points assigned to cluster k\n",
    "            cluster_data = X_train[self.clusters == i]\n",
    "            # Compute the covariance matrix for the cluster\n",
    "            if len(cluster_data) > 1:\n",
    "                cov_matrix = np.cov(cluster_data, rowvar=False)\n",
    "            else:\n",
    "                cov_matrix = np.zeros((X_train.shape[1], X_train.shape[1]))  # Handle small cluster cases\n",
    "            covariances.append(cov_matrix)\n",
    "        return np.array(covariances)\n",
    "    \n",
    "    def get_priors(self, X_train, k):\n",
    "        priors = []\n",
    "        for i in range(k):\n",
    "            # Get points assigned to cluster k\n",
    "            cluster_data = X_train[self.clusters == i]\n",
    "            # Compute the class prior\n",
    "            prior = (cluster_data.shape[0]+1)/(X_train.shape[0]+k)\n",
    "            priors.append(prior)\n",
    "        return np.array(priors)\n",
    "    \n",
    "    #return clusters\n",
    "    def get_clusters(self):\n",
    "        return self.clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, max_epochs=100, var_smoothing = 1e-9):\n",
    "        # Set model parameters\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.max_epochs = max_epochs\n",
    "        self.means = None\n",
    "        self.cov_matrices = None\n",
    "        self.class_priors = None\n",
    "        self.resp = None\n",
    "    \n",
    "    def fit(self, X_train, k):\n",
    "        # Convert to numpy array, choose k data points to initialize centroids\n",
    "        X_train = X_train.to_numpy()\n",
    "        kmeans_approx = KMeans()\n",
    "        kmeans_approx.fit(X_train, k)\n",
    "        \n",
    "        self.means = kmeans_approx.get_centroids()\n",
    "        self.cov_matrices = kmeans_approx.get_cov_matrices()\n",
    "        self.class_priors = kmeans_approx.get_priors()\n",
    "        \n",
    "        prev_log_likelihood = None\n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            # E step\n",
    "            inv_cov_matrices = np.linalg.inv(self.cov_matrices + self.var_smoothing * np.eye(X_train.shape[1]))\n",
    "\n",
    "            diff = X_train[:, np.newaxis, :] - self.means\n",
    "            quadratic_form = np.einsum('ijk,ikl,ijl->ij', diff, inv_cov_matrices, diff)\n",
    "\n",
    "            # Compute log-likelihoods without constant terms\n",
    "            log_likelihoods = (-0.5 * quadratic_form) + np.log(self.class_priors)\n",
    "            \n",
    "            log_sum_exp = np.log(np.sum(np.exp(log_likelihoods), axis=1, keepdims=True))\n",
    "            self.resp = np.exp(log_likelihoods - log_sum_exp)\n",
    "            \n",
    "            # Calculate the total log-likelihood for this epoch\n",
    "            total_log_likelihood = np.sum(log_sum_exp)\n",
    "\n",
    "            # Convergence check\n",
    "            if prev_log_likelihood is not None and np.allclose(total_log_likelihood, prev_log_likelihood, atol=1e-6):\n",
    "                print(f\"Converged at epoch {epoch}\")\n",
    "                break  # Stop if the total log-likelihood change is below the threshold\n",
    "    \n",
    "            prev_log_likelihood = total_log_likelihood\n",
    "    \n",
    "            # M-step\n",
    "            self.update_means(X_train)\n",
    "            self.update_priors(X_train)\n",
    "            self.update_cov_matrices(X_train)\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        X_test = X_test.to_numpy()\n",
    "        # Compute the inverse of the covariance matrices\n",
    "        inv_cov_matrices = np.linalg.inv(self.cov_matrices)\n",
    "        \n",
    "        # Compute the difference between the test data points and the means of each cluster\n",
    "        diff = X_test[:, np.newaxis, :] - self.means  # Shape: (n_samples, k, n_features)\n",
    "        \n",
    "        # Compute the quadratic form for the Mahalanobis distance\n",
    "        quadratic_form = np.einsum('ijk,ikl,ijl->ij', diff, inv_cov_matrices, diff)  # Shape: (n_samples, k)\n",
    "        \n",
    "        # Compute log-likelihoods without the constant terms\n",
    "        log_likelihoods = (-0.5 * quadratic_form) + np.log(self.class_priors)\n",
    "        \n",
    "        # Assign each sample to the cluster with the highest log-likelihood\n",
    "        predicted_clusters = np.argmax(log_likelihoods, axis=1)  # Shape: (n_samples,)\n",
    "        \n",
    "        return predicted_clusters\n",
    "            \n",
    "            \n",
    "            \n",
    "    def update_means(self,X_train):\n",
    "        weighted_sums = np.sum(self.resp[:, :, np.newaxis] * X_train[:, np.newaxis, :], axis=0)\n",
    "        # Denominator: Sum of the responsibilities for each cluster (to normalize the means)\n",
    "        responsibility_sums = np.sum(self.resp, axis=0)[:, np.newaxis]\n",
    "        # Compute the new means\n",
    "        self.means = weighted_sums / responsibility_sums\n",
    "    \n",
    "    def update_priors(self,X_train):\n",
    "        self.class_priors = np.sum(self.resp, axis=0) / X_train.shape[0]\n",
    "        \n",
    "    def update_cov_matrices(self,X_train):\n",
    "        means = self.means[np.newaxis, :, :]  # Shape: (1, k, n_features)\n",
    "        # Compute the difference between data points and means for each cluster\n",
    "        diff = X_train[:, np.newaxis, :] - means  # Shape: (n_samples, k, n_features)\n",
    "        # Compute the weighted outer product for each data point and cluster\n",
    "        # The result will be a matrix of shape (n_samples, k, n_features, n_features)\n",
    "        weighted_outer_products = (self.resp[:, :, np.newaxis, np.newaxis] * \n",
    "                                diff[:, :, :, np.newaxis] * diff[:, :, np.newaxis, :])\n",
    "        # Sum over samples to get the covariance matrix for each cluster\n",
    "        cov_matrices = np.sum(weighted_outer_products, axis=0) / np.sum(self.resp, axis=0)[:, np.newaxis, np.newaxis]\n",
    "        self.cov_matrices = cov_matrices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
